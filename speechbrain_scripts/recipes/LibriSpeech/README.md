This folder contains recipes for training speech recognition systems with LibriSpeech. 
To train a full speech recognition system the pipeline is the following:
1- Train a tokenizer. The tokenizer takes in input the training transcripts and determines the subword units that will be used for both acoustic and language model training.
2- Train a Language Model (LM). The language model takes in input long texts from available books. We have recipes with both RNN and transformer-based LMs. In both cases, the LM is used during beam search to assign different weights to different hypotheses generated by the acoustic model.
3- Train an acoustic model (AM). The acoustic model maps the input speech into a set of sub-words units. The current repository contains recipes for seq2seq (ctc+attention), transducers, and transformer-based systems.  Since training an LM can take several days, by default the recipes downloads a pre-trained LM.

Note also that this folder contains a Graphene-to-phoneme  (G2P) system that can be used to convert a sequence of characters into the corresponding sequence of phonemes.
For all the recipes, some libraries useful for just "pretraining and using" the model are provided (along with use examples). 
